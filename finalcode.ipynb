{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets jiwer PrettyTable openai-whisper ffmpeg pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.8\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {cuda_id}\")\n",
    "\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PhoWhisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_whisper = pipeline(\"automatic-speech-recognition\", model=\"vinai/PhoWhisper-large\", device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wav2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniforge3\\envs\\speech2text\\lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at nguyenvulebinh/wav2vec2-base-vietnamese-250h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at nguyenvulebinh/wav2vec2-base-vietnamese-250h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\ProgramData\\miniforge3\\envs\\speech2text\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wav2vec = pipeline(\"automatic-speech-recognition\", model=\"nguyenvulebinh/wav2vec2-base-vietnamese-250h\", device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('valTest.csv')\n",
    "valTest = []\n",
    "for index, row in df.iterrows():\n",
    "  obj = dict()\n",
    "  obj['text'] = row['text']\n",
    "  obj['file'] = row['file']\n",
    "  obj['path'] = row['path']\n",
    "  valTest.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6210"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6210 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  0%|          | 10/6210 [41:03<468:35:14, 272.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  0%|          | 25/6210 [1:52:28<463:47:01, 269.95s/it]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for test in tqdm(valTest):\n",
    "  row = []\n",
    "  row.append(test['text'])\n",
    "  row.append(whisper(test['path'])['text'])\n",
    "  row.append(pho_whisper(test['path'])['text'])\n",
    "  row.append(wav2vec(test['path'])['text'])\n",
    "  result.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=['Actual', \"Whisper\", \"PhoWhisper\", \"Wav2Vec\"])\n",
    "df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6210"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_wer = 0\n",
    "whisper_cer = 0\n",
    "for ref, hyp in zip(df['Actual'], df['Whisper']):\n",
    "  whisper_wer += jiwer.wer(ref, hyp)\n",
    "  whisper_cer += jiwer.cer(ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_whisper_wer = 0\n",
    "pho_whisper_cer = 0\n",
    "for ref, hyp in zip(df['Actual'], df['PhoWhisper']):\n",
    "  pho_whisper_wer += jiwer.wer(ref, hyp)\n",
    "  pho_whisper_cer += jiwer.cer(ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_wer = 0\n",
    "wav2vec_cer = 0\n",
    "for ref, hyp in zip(df['Actual'], df['Wav2Vec']):\n",
    "  wav2vec_wer += jiwer.wer(ref, hyp)\n",
    "  wav2vec_cer += jiwer.cer(ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabel for WER and CER\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"WER\", \"CER\"]\n",
    "table.add_row([\"Whisper\", round(whisper_wer/len(df), 3), round(whisper_cer/len(df), 3)])\n",
    "table.add_row([\"PhoWhisper\", round(pho_whisper_wer/len(df),3), round(pho_whisper_cer/len(df),3)])\n",
    "table.add_row([\"Wav2Vec\", round(wav2vec_wer/len(df),3), round(wav2vec_cer/len(df),3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Model</th>\n",
       "            <th>WER</th>\n",
       "            <th>CER</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Whisper</td>\n",
       "            <td>0.776</td>\n",
       "            <td>0.606</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>PhoWhisper</td>\n",
       "            <td>0.26</td>\n",
       "            <td>0.203</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Wav2Vec</td>\n",
       "            <td>0.272</td>\n",
       "            <td>0.199</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------+-------+-------+\n",
       "|   Model    |  WER  |  CER  |\n",
       "+------------+-------+-------+\n",
       "|  Whisper   | 0.776 | 0.606 |\n",
       "| PhoWhisper |  0.26 | 0.203 |\n",
       "|  Wav2Vec   | 0.272 | 0.199 |\n",
       "+------------+-------+-------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.DataFrame([[\"Whisper\", round(whisper_wer/len(df), 3), round(whisper_cer/len(df), 3)],\n",
    "                    [\"PhoWhisper\", round(pho_whisper_wer/len(df),3), round(pho_whisper_cer/len(df),3)],\n",
    "                    [\"Wav2Vec\", round(wav2vec_wer/len(df),3), round(wav2vec_cer/len(df),3)]], columns=['Model', 'WER', 'CER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.to_csv('wer_cer_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 1\n",
      "Actual: không che mặt và tay chân của trẻ tùy thời tiết cho phép mẹ trong thai kỳ nên\n",
      "Whisper:  Hãy subscribe cho kênh Ghiền Mì Gõ Để không bỏ lỡ những video hấp dẫn\n",
      "PhoWhisper: không che mặt và tay chân của trẻ tuỳ thời tiết cho phép mẹ trong thai kỳ.\n",
      "Wav2Vec: không che mặt và tay chân của trẻ tùy thời tiết cho phép mẹ trong thai kỳ\n",
      "\n",
      "Sample: 2\n",
      "Actual: rồi thứ hai lên đừng có kỳ thị tôi quá hãy coi tôi cũng giống như những người khác thôi chứ đừng có cho là tôi là mập\n",
      "Whisper:  Rồi thứ hai là đừng có kỳ thị tôi quá Hãy coi tôi cũng giống như những người khác thôi Chứ đừng có cho tôi là mập địch ú\n",
      "PhoWhisper: rồi thứ hai là đừng có kỳ thị tôi quá hãy coi tôi cũng giống như những người khác thôi chớ đừng có cho là tôi là mập địch uống.\n",
      "Wav2Vec: rồi thứ hai là đừng có kỳ thị tôi quá hay coi tôi là cũng giống như những người khác thôi chứ đừng có cho là tôi là mập địch uống\n",
      "\n",
      "Sample: 3\n",
      "Actual: người bạn đồng hành đáng tin cậy mà lại rất rẻ tiền rất là hợp lý đúng không ạ cái thứ hai nữa là gì ạ cái thứ hai nữa\n",
      "Whisper:  Các bạn hãy đăng kí cho kênh lalaschool Để không bỏ lỡ những video hấp dẫn\n",
      "PhoWhisper: lại là một cái người bạn đồng hành đáng tin cậy mà lại rất rẻ tiền rất là hợp lý đúng không ạ cái thứ hai nữa là gì ạ cái thứ.\n",
      "Wav2Vec: lại là một cái người bạn đồng hành đáng tin cậy mà lại rất rẻ tiền rất là hợp lý đúng không cái thứ hai nữa là gì ạ cái\n",
      "\n",
      "Sample: 4\n",
      "Actual: một số trường hợp bệnh nhân dùng những thuốc thuốc chống đông\n",
      "Whisper:  Một số trường hợp là bệnh nhân dùng những thuốc chống đông\n",
      "PhoWhisper: một số trường hợp là bệnh nhân dùng những thuốc chống đông.\n",
      "Wav2Vec: vâng dạ một số trường hợp là bệnh nhân dùng những thuốc thuốc chống đông\n",
      "\n",
      "Sample: 5\n",
      "Actual: thuốc tây thuốc bắc thuốc các thảo dược và vân vân đó khi chúng ta cũng\n",
      "Whisper:  thuốc Tây thuốc Bắc thuốc các thảo dược vân vân đó\n",
      "PhoWhisper: à thuốc ờ thuốc tây à thuốc bắc à thuốc à các thảo dược và vân vân đó thì chú.\n",
      "Wav2Vec: ờ thuốc thuốc tây à thuốc bắt thuốc  các thảo dược à vân vân đóthì\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  num = random.randint(0, len(df))\n",
    "  print(f\"Sample: {_+1}\")\n",
    "  print(f\"Actual: {df['Actual'][num]}\")\n",
    "  print(f\"Whisper: {df['Whisper'][num]}\")\n",
    "  print(f\"PhoWhisper: {df['PhoWhisper'][num]}\")\n",
    "  print(f\"Wav2Vec: {df['Wav2Vec'][num]}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech2text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
